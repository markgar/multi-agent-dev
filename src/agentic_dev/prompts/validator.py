"""Validator prompt templates."""

VALIDATOR_PLAYWRIGHT_SECTION = (
    "\nUI TESTING WITH PLAYWRIGHT:\n"
    "This project has a frontend. In addition to curl-based API tests, use Playwright "
    "to verify that UI pages actually render and behave correctly in a real browser.\n\n"
    "PLAYWRIGHT SETUP:\n"
    "- Add a `playwright` service to docker-compose.yml using the official Microsoft "
    "image: `mcr.microsoft.com/playwright:v1.52.0-noble`. Mount the repo so test "
    "files are accessible inside the container.\n"
    "- The playwright service must be on the same Docker network as the app service "
    "so it can reach the app by service name (e.g. `http://app:8080`).\n"
    "- Create a `e2e/` directory in the repo root for Playwright tests if it does not "
    "already exist.\n"
    "- Create `e2e/playwright.config.ts` if it does not exist:\n"
    "  ```\n"
    "  import {{ defineConfig }} from '@playwright/test';\n"
    "  export default defineConfig({{\n"
    "    testDir: '.',\n"
    "    timeout: 30000,\n"
    "    use: {{\n"
    "      baseURL: process.env.BASE_URL || 'http://app:8080',\n"
    "      headless: true,\n"
    "    }},\n"
    "  }});\n"
    "  ```\n"
    "- Create `e2e/package.json` if it does not exist:\n"
    "  ```\n"
    "  {{ \"devDependencies\": {{ \"@playwright/test\": \"^1.52.0\" }} }}\n"
    "  ```\n\n"
    "WRITING PLAYWRIGHT TESTS:\n"
    "- Write TypeScript test files in `e2e/` (e.g. `e2e/ui-validation.spec.ts`).\n"
    "- Use `data-testid` selectors where available: "
    "`page.getByTestId('login-form')`. Fall back to semantic selectors: "
    "`page.getByRole('button', {{ name: 'Submit' }})`.\n"
    "- Test these categories for each UI milestone:\n"
    "  1. Page loads without JavaScript errors (listen for `page.on('pageerror')`)\n"
    "  2. Key elements are visible (headings, navigation, forms, tables)\n"
    "  3. Navigation works (click links, verify URL changes and correct page loads)\n"
    "  4. Forms submit (fill inputs, click submit, verify success feedback)\n"
    "  5. Interactive elements respond (buttons, dropdowns, modals)\n"
    "- Keep tests focused — one behavior per test. Aim for 5-15 tests per UI "
    "milestone.\n\n"
    "RUNNING PLAYWRIGHT TESTS:\n"
    "- Install deps and run tests inside the playwright container:\n"
    "  `docker compose run --rm playwright sh -c "
    "'cd /app/e2e && npm install && npx playwright test --reporter=list'`\n"
    "- If tests fail, capture the error output and include it in validation-results.txt.\n"
    "- Include Playwright results in validation-results.txt with a [UI] prefix:\n"
    "  PASS  [UI] Login page renders with email input and submit button\n"
    "  PASS  [UI] Navigation to /members shows members table\n"
    "  FAIL  [UI] Submit member form -> expected success toast, got JS error\n"
    "- Report UI failures by creating a GitHub issue: "
    "`gh issue create --title '[UI] <description>' --body '<details>' --label bug`.\n\n"
    "IMPORTANT: Playwright tests complement curl tests — do not replace API tests "
    "with browser tests. Test APIs with curl AND test UI rendering with Playwright.\n"
)


VALIDATOR_PLAYWRIGHT_TRACE_SECTION = (
    "\nPLAYWRIGHT TRACE & REPORT CAPTURE:\n"
    "- When running Playwright tests, use both --trace on and --reporter=html "
    "in addition to --reporter=list so that detailed traces and an HTML report "
    "are captured.\n"
    "- Run the tests like this:\n"
    "  `docker compose run --rm playwright sh -c "
    "'cd /app/e2e && npm install && npx playwright test "
    "--reporter=list --reporter=html --trace on'`\n"
    "- After tests finish, the HTML report will be at `e2e/playwright-report/` "
    "and traces at `e2e/test-results/`. These directories must be on the "
    "mounted volume so they are accessible from the host.\n"
    "- Do NOT commit these artifacts — they are for local analysis only.\n"
)


VALIDATOR_JOURNEY_SECTION = (
    "\nUSER JOURNEY TESTS:\n"
    "Instead of isolated endpoint checks, run the following multi-step user journeys. "
    "Each journey navigates the app the way a real user would — creating data in one "
    "area and verifying it flows to another.\n\n"
    "For each journey below:\n"
    "1. Follow the steps IN ORDER. Each step depends on the previous step's state.\n"
    "2. Translate the natural-language steps into concrete actions — curl requests "
    "for API endpoints, or Playwright interactions for UI pages.\n"
    "3. Verify each step succeeds before moving to the next. If a step fails, "
    "log the failure and continue to the next journey.\n"
    "4. Log results in validation-results.txt with the journey ID:\n"
    "   PASS  [J-1] Admin navigates all sidebar links\n"
    "   FAIL  [J-3] Step 'create project' returned 500 instead of 201\n\n"
    "JOURNEYS TO RUN:\n"
    "{journey_list}\n"
    "For any journey failure, create a GitHub issue: "
    "`gh issue create --title '[journey] <journey-id>: <failing step>' "
    "--body '<expected vs actual behavior, milestone: {milestone_name}>' --label bug`.\n"
)


VALIDATOR_MILESTONE_PROMPT = (
    "You are a deployment validator. Your job is to build the application in a Docker "
    "container, run it, and verify it works against the acceptance criteria in SPEC.md.\n\n"
    "COMMIT MESSAGE FORMAT: Prefix ALL commit messages with '[validator]' — for example: "
    "'[validator] Add Dockerfile and docker-compose config'. This identifies you as the "
    "author in the git history.\n\n"
    "FIRST: Read DEPLOY.md if it exists — it contains everything previous runs learned "
    "about building and deploying this application. Follow its instructions for "
    "Dockerfile configuration, environment variables, port mappings, startup sequence, "
    "and known gotchas. This is your most important input after SPEC.md.\n\n"
    "Read SPEC.md for acceptance criteria. Read only the current milestone file "
    "for milestone '{milestone_name}' — you should test only the functionality added "
    "by this milestone, not previous milestones.\n\n"
    "IMPORTANT: The current milestone file has a `> **Validates:**` blockquote "
    "immediately after the `## Milestone:` heading. This block tells you EXACTLY "
    "what to test for this milestone — endpoint paths, HTTP methods, expected status "
    "codes, pages that should render, CLI commands. Read only the current milestone "
    "file's > **Validates:** blockquote as your test plan. Do NOT read or re-test "
    "previous milestones.\n\n"
    "Run `git diff {milestone_start_sha} "
    "{milestone_end_sha} --name-only` to see what changed in this milestone.\n\n"
    "PORT ISOLATION:\n"
    "- Before running ANY docker compose commands, run: "
    "`export COMPOSE_PROJECT_NAME={compose_project_name}`\n"
    "- Use host port {app_port} for the main application service. In docker-compose.yml "
    "port mappings, use `{app_port}:<container-port>` instead of the default host port "
    "(e.g. `{app_port}:8080` not `8080:8080`). If a separate frontend service exists, "
    "use host port {secondary_port}.\n"
    "- COMPOSE_PROJECT_NAME ensures this project's containers are namespaced and won't "
    "conflict with other projects running on the same host.\n\n"
    "CONTAINER SETUP:\n"
    "- First, stop and remove any running containers from THIS project's previous "
    "validation: `docker compose down --remove-orphans`.\n"
    "- If no Dockerfile exists, create one appropriate for the project's tech stack. "
    "If the app needs a database or other services, create a docker-compose.yml.\n"
    "- If no .dockerignore exists, create one to exclude directories that bloat the "
    "build context: node_modules, .git, bin, obj, dist, __pycache__, .venv, etc. "
    "This prevents unnecessary files from being sent to the Docker daemon and avoids "
    "stale dependency cache issues.\n"
    "- Build the container: `docker compose build` (or `docker build -t app .`).\n"
    "- Start the container: `docker compose up -d` (or `docker run -d`). "
    "Use the host port mappings specified above.\n"
    "- Wait for the app to be healthy — retry curl against the main endpoint for up to "
    "30 seconds with short sleeps between attempts.\n\n"
    "{validation_scope}"
    "{ui_testing_instructions}"
    "AFTER TESTING — LEAVE CONTAINERS RUNNING:\n"
    "- Do NOT tear down containers after testing. Leave them running so the "
    "application remains accessible at http://localhost:{app_port} for browsing.\n"
    "- Do NOT run `docker compose down`, `docker stop`, or `docker rm` after tests.\n"
    "- Containers will be cleaned up automatically before the next milestone's "
    "validation.\n\n"
    "VALIDATION RESULTS LOG:\n"
    "- After running all tests, write a file called `validation-results.txt` in the "
    "repo root. Do NOT commit this file — it is for logging only.\n"
    "- Format: one line per test. Each line: `PASS` or `FAIL` followed by a short "
    "description of what was tested and key details (method, endpoint, status code, "
    "expected vs actual if failed).\n"
    "- Tag each line with its check type so the log shows which part of the "
    "validation produced it:\n"
    "{results_tag_instructions}"
    "- Include container build and startup as untagged test lines too:\n"
    "  PASS  docker compose build -> success\n"
    "  PASS  container startup -> healthy after 2s\n"
    "- This file is overwritten each milestone (not appended).\n\n"
    "REPORTING:\n"
    "- For any failure — container won't build, app won't start, endpoint returns "
    "wrong data, error case not handled — create a GitHub issue: "
    "`gh issue create --title '[bug] <short description>' "
    "--body '<what failed, expected vs actual, milestone: {milestone_name}>' "
    "--label bug`. "
    "Before creating, check for duplicates: "
    "`gh issue list --label bug --state open --json number,title` and skip if a "
    "similar issue already exists.\n\n"
    "DEPLOY.md UPDATE (critical):\n"
    "- After you finish (whether tests pass or fail), update DEPLOY.md with everything "
    "you learned about deploying this application. Include:\n"
    "  - Dockerfile location and any build arguments or multi-stage notes\n"
    "  - Required environment variables and their values\n"
    "  - Port mappings (container port → host port)\n"
    "  - Docker Compose service configuration if applicable\n"
    "  - Startup sequence (e.g. database must start before app, migrations needed)\n"
    "  - Health check endpoint and expected response\n"
    "  - Known gotchas and fixes discovered during this run\n"
    "- Be specific and actionable — the next milestone's validator will rely on this "
    "file to get the container running quickly.\n"
    "- If DEPLOY.md already exists, preserve existing content and add new findings. "
    "Update any information that has changed (e.g. new env vars, different ports).\n\n"
    "Commit all changes (Dockerfile, docker-compose.yml, DEPLOY.md) with a "
    "descriptive message prefixed with '[validator]'. "
    "Do NOT run git push — the orchestration system handles pushing after "
    "returning to the main branch. Just commit locally and stop."
)


# ============================================
# Validation scope blocks (interpolated into VALIDATOR_MILESTONE_PROMPT)
# ============================================


VALIDATOR_LEGACY_SCOPE = (
    "ACCEPTANCE TESTING (three-part scope):\n\n"
    "A. CURRENT MILESTONE VALIDATION:\n"
    "- Read only the current milestone file's > **Validates:** block. Test those "
    "endpoints, pages, and behaviors.\n"
    "- Do NOT read or re-test previous milestones.\n"
    "- For the first milestone, this is typically just: the app starts in a container "
    "and responds to a basic request (health check, root endpoint, --help, etc.).\n"
    "- For later milestones, test only what this milestone added: hit the relevant "
    "API endpoints with valid and invalid data, verify status codes and response "
    "bodies, test error cases, verify CRUD operations.\n"
    "- For CLI tools, run the relevant commands inside the container and verify output.\n\n"
    "B. REQUIREMENTS COVERAGE:\n"
    "- Read REQUIREMENTS.md. Read BACKLOG.md to find which story this milestone "
    "('{milestone_name}') belongs to.\n"
    "- Identify the requirements from REQUIREMENTS.md that align with that story's "
    "description.\n"
    "- Verify each of those requirements is working in the running app.\n"
    "- For any requirement that should be deliverable by this story but isn't, "
    "create a GitHub issue: `gh issue create --title '[missing-requirement] <description>' "
    "--body '<what was required and what is missing>' --label bug`.\n\n"
)

VALIDATOR_LEGACY_RESULTS_TAGS = (
    "  [A] for current milestone validation tests\n"
    "  [B] for requirements coverage checks\n"
    "- Example lines:\n"
    "  PASS  docker compose build -> success\n"
    "  PASS  container startup -> healthy after 2s\n"
    "  PASS  [A] GET /health -> 200 {{\"status\":\"healthy\"}}\n"
    "  PASS  [A] GET /books -> 200 returned 3 books\n"
    "  FAIL  [A] POST /books with missing title -> expected 400, got 500\n"
    "  PASS  [B] Requirement 'list all books' -> GET /books returns collection\n"
    "  FAIL  [B] [missing-requirement] Requirement 'filter books by genre' -> "
    "no query param support found\n"
)

VALIDATOR_JOURNEY_RESULTS_TAGS = (
    "  [J-N] for journey test results (use the journey ID)\n"
    "- Example lines:\n"
    "  PASS  docker compose build -> success\n"
    "  PASS  container startup -> healthy after 2s\n"
    "  PASS  [J-1] Admin navigates all sidebar links -> all pages loaded\n"
    "  FAIL  [J-3] Step 'create project' -> POST /api/projects returned 500\n"
    "  PASS  [J-7] Admin creates event at venue -> event visible in list\n"
)
